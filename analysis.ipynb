{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# simulation_root = \"C:/kerta/AppData/LocalLow/DefaultCompany/Sumobot/Simulation\"\n",
    "# output_folder = \"D:/Simulation_CSV\"\n",
    "simulation_root = \"/Users/defdef/Library/Application Support/DefaultCompany/Sumobot/Simulation\"\n",
    "csv_folder = \"/Users/defdef/Documents/Simulation\"\n",
    "\n",
    "batch_checkpoint_dir = \"batched\"\n",
    "summarized_dir = \"result\"\n",
    "arena_heatmaps_output = \"result/arena_heatmaps\"\n",
    "\n",
    "os.makedirs(batch_checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(summarized_dir, exist_ok=True)\n",
    "os.makedirs(arena_heatmaps_output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Simulation Log to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compile.log_to_csv import ( \n",
    "    convert_all_configs\n",
    ")\n",
    "\n",
    "convert_all_configs(simulation_root, csv_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summarization CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compile.generator_polars_gpu import (\n",
    "        batch_process_csvs,\n",
    "        generate_timebins_from_batches,\n",
    "        generate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Batched CSV\n",
    "\n",
    "Process CSVs in batches and save checkpoints\n",
    "\n",
    "Structure: base_dir/BotA_vs_BotB/ConfigFolder/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timebin_size = 5\n",
    "batch_size = 2 # if there's 156 matchup simulation folder, it will generate 156 / 2 = 78 summarization batch csv\n",
    "\n",
    "batch_process_csvs(\n",
    "    csv_folder, \n",
    "    batch_size=batch_size,\n",
    "    time_bin_size=timebin_size,\n",
    "    checkpoint_dir=batch_checkpoint_dir,\n",
    "    compute_timebins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Final Summarization CSV from Batches\n",
    "\n",
    "Generate timebin summaries from batched timebin checkpoints\n",
    "Loads batch files and creates final summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(batch_checkpoint_dir, summarized_dir) # generate summarization csv\n",
    "generate_timebins_from_batches(batch_checkpoint_dir, summarized_dir) # generate csv containing batched timebins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Arena Heatmaps Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from compile.generate_arena_heatmap import ( \n",
    "    create_phased_heatmaps_all_bots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run to Generate Arena Heatmap figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "create_phased_heatmaps_all_bots(\n",
    "            csv_folder,\n",
    "            output_dir = arena_heatmaps_output,\n",
    "            actor_position=\"both\",\n",
    "            chunksize=50000,\n",
    "            max_configs=None,  # None for all configs, only fill to test, e.g. 2 or 5 configs\n",
    "            mode=\"all\",  # Generate both heatmaps and position distributions\n",
    "            use_timer=False, # Group by existing Timer configuration\n",
    "            use_time_windows=True, # Use time windows [skip_initial-15, 15-30, 30-45, 45-60]\n",
    "            include_distance_over_time=True,  \n",
    "            skip_initial=2.5\n",
    "        )\n",
    "\n",
    "elapsed_seconds = time.time() - start\n",
    "hours, remainder = divmod(elapsed_seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "processing_time = f\"{int(hours):02d}:{int(minutes):02d}:{seconds:.2f}\"\n",
    "print(f\"\\nProcessing Time: {processing_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from plotting.overall_analyzer import (\n",
    "        plot_action_radar,\n",
    "        plot_collision_radar,\n",
    "        plot_winrate_matrix,\n",
    "        plot_overall_bot_metrics,\n",
    "        plot_grouped_config_winrates,\n",
    "        plot_time_related,\n",
    "        plot_action_distribution_stacked,\n",
    "        plot_action_timebins_intensity,\n",
    "        plot_collision_timebins_intensity,\n",
    "        plot_collision_distribution_stacked,\n",
    "        plot_action_win_related,\n",
    "        plot_all_correlations,\n",
    "        plot_full_cross_heatmap_half,\n",
    "    )\n",
    "    \n",
    "from plotting.individual_analyzer import (\n",
    "    plot_individual_bot_correlations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load summary data\n",
    "df_sum = pd.read_csv(f\"{summarized_dir}/summary_bot.csv\").rename(columns={\"Duration\": \"Duration (ms)\"})\n",
    "df = pd.read_csv(f\"{summarized_dir}/summary_matchup.csv\")\n",
    "df_timebins = pd.read_csv(f\"{summarized_dir}/summary_action_timebins.csv\")\n",
    "df_collision_timebins = pd.read_csv(f\"{summarized_dir}/summary_collision_timebins.csv\")\n",
    "\n",
    "# Configuration\n",
    "cfg = {\n",
    "    \"Timer\": sorted(df[\"Timer\"].unique().tolist()),\n",
    "    \"ActInterval\": sorted(df[\"ActInterval\"].unique().tolist()),\n",
    "    \"Round\": sorted(df[\"Round\"].unique().tolist()),\n",
    "    \"SkillLeft\": sorted(df[\"SkillLeft\"].unique().tolist()),\n",
    "    \"SkillRight\": sorted(df[\"SkillRight\"].unique().tolist()),\n",
    "    \"Bots\": sorted(df[\"Bot_L\"].unique().tolist()),\n",
    "}\n",
    "bots = str.join(\", \", cfg[\"Bots\"])\n",
    "\n",
    "# Display settings\n",
    "width = 10\n",
    "height = 6\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"\\nBots in experiment: {bots}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Matchup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Matchup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Analysis\n",
    "\n",
    "Analyze bot agents facing other agents with similar configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bot Behaviour Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions Behaviour\n",
    "Mean action counts per bot across all configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_action_radar(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collision Behaviour\n",
    "Hit/Struck/Tie distribution per bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_collision_radar(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Matrix\n",
    "\n",
    "Shows how often each bot wins against others across different matchups.\n",
    "This is calculated with taking mean of each configuration (10-games iteration matchup) resulting 240 games in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_winrate_matrix(df, width, height)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_overall_bot_metrics(df, metric=\"ActionCounts_L\", title=\"Mean Action per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_overall_bot_metrics(df, metric=\"Duration_L\", title=\"Mean Action Duration per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_overall_bot_metrics(df, metric=\"Collisions_L\", title=\"Mean Collisions per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration (All Configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_overall_bot_metrics(df, metric=\"MatchDur\", title=\"Mean Match Duration per Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Win Rate Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Collisions_L\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"ActionCounts_L\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Duration Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"Duration_L\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"Timer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Action Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"ActInterval\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"Round\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Duration Grouped by Skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_grouped_config_winrates(df, metric=\"MatchDur\", config_col=\"Skill\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Related Trends\n",
    "\n",
    "Analyzes Bots aggressiveness over game duration with determining how much action taken duration related to the overall game duration (Time Setting).\n",
    "Higher timers don't always lead to longer matches. Some matchups finish fights early regardless of time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = plot_time_related(df, width, height)\n",
    "for fig in figs:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Distribution per Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_action_distribution_stacked(df, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Intensity Over Time (Per Configuration)\n",
    "\n",
    "Shows action intensity over time for different timer and action interval configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timI in cfg[\"Timer\"]:\n",
    "    for actI in cfg[\"ActInterval\"]:\n",
    "        print(f\"\\n--- Timer={timI}, ActionInterval={actI} ---\")\n",
    "        \n",
    "        # Total action intensity\n",
    "        fig = plot_action_timebins_intensity(df_timebins, timer=timI, act_interval=actI, mode=\"total\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Per-action intensity\n",
    "        fig = plot_action_timebins_intensity(df_timebins, timer=timI, act_interval=actI, mode=\"per_action\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Intensity Over All Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total action intensity\n",
    "fig = plot_action_timebins_intensity(df_timebins, mode=\"total\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-action intensity\n",
    "fig = plot_action_timebins_intensity(df_timebins, mode=\"per_action\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Intensity Over Time (Per Configuration)\n",
    "\n",
    "Shows collision intensity over time for different timer and action interval configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timI in cfg[\"Timer\"]:\n",
    "    for actI in cfg[\"ActInterval\"]:\n",
    "        print(f\"\\n--- Timer={timI}, ActionInterval={actI} ---\")\n",
    "        \n",
    "        # Total collision intensity\n",
    "        fig = plot_collision_timebins_intensity(df_collision_timebins, timer=timI, act_interval=actI, mode=\"total\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()\n",
    "        \n",
    "        # Per-type collision intensity\n",
    "        fig = plot_collision_timebins_intensity(df_collision_timebins, timer=timI, act_interval=actI, mode=\"per_type\", summary_df=df)\n",
    "        if fig:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Detail Distribution per Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_collision_distribution_stacked(df, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision Intensity Over All Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total collision intensity\n",
    "fig = plot_collision_timebins_intensity(df_collision_timebins, mode=\"total\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-type collision intensity\n",
    "fig = plot_collision_timebins_intensity(df_collision_timebins, mode=\"per_type\", timer=60, summary_df=df)\n",
    "if fig:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Taken vs. Win Relation\n",
    "\n",
    "Does spending most action (aggressive) lead to a win?\n",
    "This taking mean of action-taken per games versus win-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_action_win_related(df, width, height)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Analysis (Overall)\n",
    "\n",
    "Correlation analysis using Pearson coefficient with scatter plots and regression lines.\n",
    "All data from all bots combined, separated by configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_figs = plot_all_correlations(df, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Bot Analysis\n",
    "\n",
    "Analyze bot agent against its different configurations.\n",
    "Each of report: Win Rate; Collision; Action-Taken; Duration; is calculated with averaging data from matchup (left and right position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Analysis (Per Bot)\n",
    "\n",
    "Detailed plots for individual bots, separated by configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique bots\n",
    "bots_list = sorted(df['Bot_L'].unique())\n",
    "print(f\"Analyzing {len(bots_list)} bots: {bots_list}\")\n",
    "\n",
    "# Individual bot correlation analysis\n",
    "for bot in bots_list:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing correlations for {bot}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    correlation_figs = plot_individual_bot_correlations(df, bot, width, height)\n",
    "    \n",
    "    if not correlation_figs:\n",
    "        print(f\"No data available for {bot}\")\n",
    "        continue\n",
    "    \n",
    "    # Win Rate vs ActInterval\n",
    "    if 'actinterval' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Action Interval Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Round Type\n",
    "    if 'roundtype' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Round Type Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Timer\n",
    "    if 'timer' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Timer Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Skill Type\n",
    "    if 'skilltype' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Skill Type Configuration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Action Types\n",
    "    if 'actions' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Individual Action Types ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Action Duration\n",
    "    if 'actions_dur' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Individual Action Duration ---\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Win Rate vs Collisions\n",
    "    if 'collisions' in correlation_figs:\n",
    "        print(\"\\n--- Win Rate vs Collision Types (Hit, Struck, Tie) ---\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arena Heatmaps - Bot Movement Analysis\n",
    "\n",
    "Visualize bot movement patterns across different game phases (Early, Mid, Late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(arena_heatmaps_output):\n",
    "    # Get all bot directories\n",
    "    bot_dirs = [d for d in os.listdir(arena_heatmaps_output)\n",
    "               if os.path.isdir(os.path.join(arena_heatmaps_output, d))]\n",
    "    \n",
    "    # Sort bot directories by rank from df_sum\n",
    "    if \"Rank\" in df_sum.columns and \"Bot\" in df_sum.columns:\n",
    "        rank_map = df_sum.groupby(\"Bot\")[\"Rank\"].first().to_dict()\n",
    "        bot_dirs = sorted(bot_dirs, key=lambda b: rank_map.get(b, 9999))\n",
    "    else:\n",
    "        bot_dirs = sorted(bot_dirs)\n",
    "    \n",
    "    if bot_dirs:\n",
    "        phase_names = [\"window_2.5-15s.png\", \"window_15-30s.png\", \"window_30-45s.png\", \"window_45-60s.png\"]\n",
    "        \n",
    "        # Display heatmaps for each bot\n",
    "        for bot_name in bot_dirs:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"{bot_name} (#{bot_dirs.index(bot_name)+1})\")\n",
    "            print(f\"{'='*60}\")\n",
    "            bot_dir = os.path.join(arena_heatmaps_output, bot_name)\n",
    "            \n",
    "            # Display phase heatmaps\n",
    "            fig, axes = plt.subplots(1, len(phase_names), figsize=(20, 5))\n",
    "            for idx, phase_name in enumerate(phase_names):\n",
    "                image_path = os.path.join(bot_dir, phase_name)\n",
    "                if os.path.exists(image_path):\n",
    "                    image = Image.open(image_path)\n",
    "                    axes[idx].imshow(image)\n",
    "                    axes[idx].set_title(phase_name)\n",
    "                    axes[idx].axis('off')\n",
    "                else:\n",
    "                    axes[idx].text(0.5, 0.5, f\"Image not found:\\n{phase_name}\",\n",
    "                                  ha='center', va='center')\n",
    "                    axes[idx].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display position distribution\n",
    "            dist_path = os.path.join(bot_dir, \"position_distribution.png\")\n",
    "            if os.path.exists(dist_path):\n",
    "                print(\"\\nPosition Distribution (X & Y Overlayed)\")\n",
    "                dist_image = Image.open(dist_path)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(dist_image)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            \n",
    "            # Display distance distribution\n",
    "            dist_path = os.path.join(bot_dir, \"distance_distribution.png\")\n",
    "            if os.path.exists(dist_path):\n",
    "                print(\"\\nDistance Distribution\")\n",
    "                dist_image = Image.open(dist_path)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(dist_image)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "            print(\"\\nFull Configuration Analysis\")\n",
    "            fig = plot_full_cross_heatmap_half(df, bot_name=bot_name)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No bot heatmaps found in directory\")\n",
    "        print(\"Run: `python detailed_analyzer.py all` to generate heatmaps\")\n",
    "else:\n",
    "    print(f\"Heatmap directory not found: {arena_heatmaps_output}\")\n",
    "    print(\"Run: `python detailed_analyzer.py all` to generate heatmaps for all bots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
